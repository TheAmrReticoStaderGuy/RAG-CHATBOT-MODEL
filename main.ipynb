{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSyEKeWaEEUT",
        "outputId": "a3273032-69be-436e-fb4e-e9752857d7d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# Initialize the Hugging Face Hub\n",
        "hub_llm = HuggingFaceHub(\n",
        "    huggingfacehub_api_token=os.environ['HUGGINGFACEHUB_API_TOKEN'],\n",
        "    repo_id='meta-llama/Meta-Llama-3-8B-Instruct',\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 10000, \"min_length\": 5000}\n",
        ")\n",
        "\n",
        "# Streamlit file uploader\n",
        "uploaded_files = st.file_uploader(\"Choose PDF files\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    documents = []\n",
        "    for uploaded_file in uploaded_files:\n",
        "        # Get the original filename\n",
        "        original_filename = uploaded_file.name\n",
        "\n",
        "        # Save the uploaded file with the original filename\n",
        "        with open(original_filename, \"wb\") as f:\n",
        "            f.write(uploaded_file.getbuffer())\n",
        "\n",
        "        # Load the saved PDF file using PyPDFLoader\n",
        "        loader = PyPDFLoader(original_filename)\n",
        "        documents.extend(loader.load())\n",
        "\n",
        "    # Split the loaded documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    # Initialize the embeddings model\n",
        "    model_name = \"BAAI/bge-base-en\"\n",
        "    encode_kwargs = {'normalize_embeddings': True}\n",
        "    model_norm = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "\n",
        "    # Create a vector store\n",
        "    persist_directory = 'db'\n",
        "    embedding = model_norm\n",
        "    vectordb = Chroma.from_documents(documents=texts, embedding=embedding, persist_directory=persist_directory)\n",
        "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "    def process_llm_response(llm_response):\n",
        "    # Extract the generated answer from the response\n",
        "    # Split the answer on the basis of 'Question:' or similar markers if necessary\n",
        "      answer = llm_response['result'].strip()\n",
        "\n",
        "    # Display the generated answer\n",
        "      st.write(answer)\n",
        "\n",
        "    # Display sources\n",
        "      st.write('\\nSources:')\n",
        "      unique_sources = set()\n",
        "      for source in llm_response[\"source_documents\"]:\n",
        "        if 'source' in source.metadata:\n",
        "            unique_sources.add(source.metadata['source'])\n",
        "        elif 'file_name' in source.metadata:\n",
        "            unique_sources.add(source.metadata['file_name'])\n",
        "        else:\n",
        "            unique_sources.add(\"Unknown source\")\n",
        "\n",
        "    # Display each unique source\n",
        "      if unique_sources:\n",
        "        for source in unique_sources:\n",
        "            st.write(source)\n",
        "      else:\n",
        "        st.write(\"None\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "    DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "    You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "    If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "    def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT):\n",
        "        SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "        prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "        return prompt_template\n",
        "\n",
        "    sys_prompt = \"\"\"you are a hepful AI assistant who answers questions correctly and precisely with the help of the intstructions/ context given. You only gieve the answer and not the entire prompt as your llm response\n",
        "    \"\"\"\n",
        "    instruction = \"\"\"CONTEXT:/n/n {context}/n\n",
        "\n",
        "    Question: {question}\"\"\"\n",
        "\n",
        "    # Create the prompt template once\n",
        "    prompt_template = get_prompt(instruction, sys_prompt)\n",
        "\n",
        "    llama_prompt = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain_type_kwargs = {\"prompt\": llama_prompt}\n",
        "\n",
        "    # Create the QA chain\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        llm=hub_llm,\n",
        "        chain_type=\"stuff\",\n",
        "        retriever=retriever,\n",
        "        chain_type_kwargs=chain_type_kwargs,\n",
        "        return_source_documents=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHMTJ0_PEIbc",
        "outputId": "ecde6e77-ee48-4625-f64c-072bbc5fff6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.106.169.182\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnRKU2CtE3HH",
        "outputId": "e0a490e0-6a35-4726-feb8-645290b60deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.106.169.182:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
